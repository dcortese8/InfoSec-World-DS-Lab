{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ur2YyjU2oYx5"
      },
      "source": [
        "# Secure by Design Data Science Workshop\n",
        "\n",
        "## Hands-on Exercise with JavaScript Vulnerability Data Set\n",
        "\n",
        "## Background\n",
        "**Problem**: Static code analysis tools like OpenStaticAnalyzer and escomplex provide various metrics on code but does not necessarily indicate if code is vulnerable as a result. Some of the metrics used (like Halstead metrics) output numerical values with ambiguous interpretation for code security.\n",
        "\n",
        "**Goal**: Explore methods to see if metrics generated by static code complexity analysis can be used to predict a function's vulnerability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsrwvfpoNFR7"
      },
      "source": [
        "## Dataset\n",
        "Dataset was derived from vulnerability information in public databases of the Node Security Platform, Snyk Vulnerability Database and code fixing patches from GitHub applied to JavaScript code.\n",
        "\n",
        "Contains a labelled dataset of 12,125 functions with 1496 vulnerable records.\n",
        "\n",
        "Code metrics are provided by OpenStaticAnalyzer and escomplex tools.\n",
        "\n",
        "Reference: Ferenc, Rudolf & Hegedüs, Péter & Gyimesi, Péter & Antal, Gabor & Bán, Dénes & Gyimothy, Tibor. (2019). Challenging Machine Learning Algorithms in Predicting Vulnerable JavaScript Functions. 8-14. 10.1109/RAISE.2019.00010."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxaU1UFUNFR7"
      },
      "source": [
        "## Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-kKI0vnNFR7"
      },
      "outputs": [],
      "source": [
        "\n",
        "import gdown\n",
        "\n",
        "file_id = \"1GfGsJ-rSdbyGwAZ7CxG8QFMw3H_mBJxL\"\n",
        "\n",
        "output_file = \"JSVulnerabilityDataSet-1.0.csv\"\n",
        "\n",
        "gdown.download(f\"https://drive.google.com/uc?id={file_id}\", output_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXRnWKOSNFR8"
      },
      "source": [
        "## Import Libraries\n",
        "- import _____ as ___ allows us to reference the package using an alias. For example, 'import pandas as pd' allows us to use pd every time we reference pandas.\n",
        "- from _____ import _____ allows us to import specific methods or objects from a package. For example, 'from sklearn.metrics import KNeighborsClassifier' imports KNeighborsClassifier from the sklearn.neighbors module.\n",
        "### Data Processing\n",
        "- 'numpy' is a low-overhead data management library using arrays.\n",
        "- 'pandas' is a high-overhead data management library that uses dataframes, which include many convenient methods for exploring and processing the data.\n",
        "- 'sklearn' is a data science library that contains many convenient methods for predictive modeling.\n",
        "\n",
        "### Data Visualization\n",
        "- 'matplotlib' is for plotting.\n",
        "- 'seaborn' is for plotting, specifically a heatmap\n",
        "- 'plotly' is also used for plotting but with a more intuitive and easy to use API at the expense of complex customizability present with matplotlib\n",
        "- ipywidgets is used to generate interactive widgets for the user\n",
        "- 'IPython.display' provides a display and clear_ouput function to help control displayed artifacts within the notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ku0hhu8XNFR8"
      },
      "outputs": [],
      "source": [
        "# Install the packages if they are not in the system.\n",
        "# If there are errors related to missing packages, delete '--quiet' at the end to view output of this step.\n",
        "%pip install numpy pandas scikit-learn seaborn matplotlib plotly ipywidgets --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rIyN9Q03Mbi"
      },
      "outputs": [],
      "source": [
        "# Data Processing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "\n",
        "# Model Training\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Model Evaluation\n",
        "from sklearn.metrics import classification_report\n",
        "### Double line import for easier reading\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, Markdown\n",
        "###########################\n",
        "###########################\n",
        "# STUDENTS ADD CODE HERE TO:\n",
        "# Import seaborn into the colab environement and use the alias sns\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "###########################\n",
        "###########################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdzKe2t5dD6A"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sn4xGXei3Q87"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(output_file, index_col=0)\n",
        "# Lowercase the column names\n",
        "df.columns = df.columns.str.lower()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLHAfofUdSGM"
      },
      "source": [
        "# Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owEObYvDNFR9"
      },
      "source": [
        "Field descriptions are as follows\n",
        "\n",
        "METRIC | DESCRIPTION | TOOL\n",
        "-------|------------|-------\n",
        "CC | Clone Coverage | OSA\n",
        "CCL | Clone Classes | OSA\n",
        "CCO | Clone Complexity | OSA\n",
        "CI  | Clone Instances | OSA\n",
        "CLC | Clone Line Coverage | OSA\n",
        "LDC | Lines of Duplicated Code | OSA\n",
        "McCC, CYCL | Cyclomatic Complexity | OSA, escomplex\n",
        "NL | Nesting Level | OSA\n",
        "NLE |Nesting Level without else-if |OSA\n",
        "CD, TCD | ($Total^2$) Comment Density | OSA\n",
        "CLOC, TCLOC | (Total) Comment Lines of Code | OSA\n",
        "DLOC | Documentation Lines of Code | OSA\n",
        "LLOC, TLLOC | (Total) Logical Lines of Code | OSA\n",
        "LOC, TLOC | (Total) Lines of Code | OSA\n",
        "NOS, TNOS | (Total) Number of Statements | OSA\n",
        "NUMPAR, PARAMS | Number of Parameters | OSA, escomplex\n",
        "HOR D | Nr. of Distinct Halstead Operators | escomplex\n",
        "HOR T | Nr. of Total Halstead Operators | escomplex\n",
        "HON D | Nr. of Distinct Halstead Operands | escomplex\n",
        "HON T | Nr. of Total Halstead Operands | escomplex\n",
        "HLEN | Halstead Length | escomplex\n",
        "HVOC | Halstead Vocabulary Size |escomplex\n",
        "HDIFF | Halstead Difficulty | escomplex\n",
        "HVOL |Halstead Volume| escomplex\n",
        "HEFF |Halstead Effort |escomplex\n",
        "HBUGS |Halstead Bugs| escomplex\n",
        "HTIME |Halstead Time| escomplex\n",
        "CYCL DENS| Cyclomatic Density |escomplex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Problem\n",
        "- Static code metrics do not necessarily reflect vulnerabilities. Consider a few examples:\n",
        "\n",
        "## Cyclomatic Complexity\n",
        "- Measure of the number of linearly independent paths through a program's source code. Also called McCabe's Complexity. \n",
        "- Counts logical forks in code.\n",
        "- $CC = E - N + 2P$\n",
        "    - E: Number of logical forks (edges) in the code\n",
        "    - N: Number of code blocks (nodes) between logical forks\n",
        "    - P: Number of connected components (1 for a single function)\n",
        "- Can be simplified to $CC = \\text{Num Decision Points} + 1$ for isolated code\n",
        "\n",
        "#### Example\n",
        "- CC = 3 Decision points + 1 = 4\n",
        "```javascript\n",
        "function validateInput(input) {\n",
        "  if (!input) {\n",
        "    return false;\n",
        "  }\n",
        "  \n",
        "  if (input.length < 2) {\n",
        "    return false;\n",
        "  }\n",
        "  \n",
        "  if (input.length > 200) {\n",
        "    return false;\n",
        "  }\n",
        "\n",
        "  return true;\n",
        "}\n",
        "```\n",
        "\n",
        "## Halstead Metrics\n",
        "- Represents metrics using operators and operands.\n",
        "    - $\\eta_1 = \\text{The number of distinct operators}$: Includes functions, operators (like &, +, (), etc.)\n",
        "    - $\\eta_2 = \\text{The number of distinct operands}$: Inlcudes variables, constants, etc.\n",
        "    - $N_1 = \\text{The total number of operators}$\n",
        "    - $N_2 = \\text{The total number of operands}$\n",
        "\n",
        "- Halstead Length = $N_1 + N_2$\n",
        "- Halstead Vocabulary = $\\eta = \\eta_1 + \\eta_2$\n",
        "- Halstead Difficulty = $D = \\frac{\\eta_1}{2}\\times \\frac{N_2}{\\eta_2}$\n",
        "- Halstead Volume = $V = N \\times log_2\\eta$\n",
        "- Halstead Effort = $E = D \\times V$\n",
        "- Halstead Bugs = $B = \\frac{V}{3000}$\n",
        "- Halstead Time = $T = \\frac{E}{18}\\text{ seconds}$\n",
        "\n",
        "#### Example\n",
        "- Operators = `main, (), {}, prompt, [], prompt, =, split, map, Math.floor, /, console.log` 12 Operators\n",
        "- Operands = `input, a, b, c, Number, avg, 3` 7 Operands\n",
        "- Total Operators = 27\n",
        "- Total Operands = 15\n",
        "\n",
        "- Halstead Length  = 42\n",
        "- Halstead Vocabulary = 19\n",
        "- Halstead Difficulty = 12.85\n",
        "- Halstead Volume = 178.4\n",
        "- Halstead Effort = 2292.44\n",
        "- Halstead Bugs = 0.05\n",
        "- Halstead Time = 127.357 seconds\n",
        "\n",
        "```javascript\n",
        "function main() {\n",
        "  // Simulate user input\n",
        "  let input = prompt(\"Enter three integers separated by spaces:\");\n",
        "  let [a, b, c] = input.split(\" \").map(Number);\n",
        "\n",
        "  let avg = Math.floor((a + b + c) / 3);\n",
        "\n",
        "  console.log(\"avg =\", avg);\n",
        "}\n",
        "```\n",
        "\n",
        "# Question\n",
        "- Do these numbers give you sufficient insight into code vulnerability?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-TKrk1HNFR9"
      },
      "source": [
        "# View first 10 Records\n",
        "'df.head()' prints the first 5 records, which provides an initial understanding of the fields, data types, and data scales. Observe that:\n",
        "\n",
        "- Some fields are strings\n",
        "- Other fields are numerical, either an integer or a float"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFAbAhUCdVV5"
      },
      "outputs": [],
      "source": [
        "###########################\n",
        "###########################\n",
        "# STUDENTS ADD CODE HERE TO:\n",
        "# Print and inspect the first 10 records\n",
        "df.head(10)\n",
        "###########################\n",
        "###########################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xzrmfiidViO",
        "outputId": "f7c87a05-726f-484e-f0e1-d8e5315fff33"
      },
      "source": [
        "# Statistics and Histogram of the raw data\n",
        "\n",
        "Observe that:\n",
        "\n",
        "- There are many different scales, and the data are not Gaussian (i.e., not normal)\n",
        "- Many fields are categorical and not numeric (despite their dtypes being int64)\n",
        "- Some fields are binary (despite their dtypes being int64)\n",
        "- Some fields contain imbalanced data\n",
        "- 'cllc' may be a mislabelled column (or could stand for LDC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "###########################\n",
        "###########################\n",
        "# STUDENTS ADD CODE HERE TO:\n",
        "# Inspect the data types of your dataframe\n",
        "df.dtypes()\n",
        "###########################\n",
        "###########################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01sAVOJn3vaI"
      },
      "outputs": [],
      "source": [
        "# Select only the numeric columns\n",
        "numeric_df = df.select_dtypes(include=np.number)\n",
        "\n",
        "# Drop positional features\n",
        "numeric_df.drop(columns=['line','column','endline','endcolumn'], axis=1, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRQt4ql--nw1"
      },
      "outputs": [],
      "source": [
        "###########################\n",
        "###########################\n",
        "# STUDENTS ADD CODE HERE TO:\n",
        "# Generate histograms of the numeric data.\n",
        "numeric_df.hist(figsize=(12, 10))\n",
        "###########################\n",
        "###########################\n",
        "plt.tight_layout()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51KF4gzLNFR-"
      },
      "source": [
        "# Data Distributions\n",
        "\n",
        "We can visualize the distribution of the vulnerable to non vulnerable functions in the dataset.\n",
        "Observet that:\n",
        "- There are more non vulnerable functions than vulnerable, thus we can define this to be an 'imbalanced' dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Rb8kFMadVbr"
      },
      "outputs": [],
      "source": [
        "###########################\n",
        "###########################\n",
        "# STUDENTS ADD CODE HERE TO:\n",
        "# Get the value counts for the 'vuln' column from the numeric data\n",
        "value_counts = numeric_df['vuln'].value_counts()\n",
        "###########################\n",
        "###########################\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJ8Br4YhNFR-"
      },
      "outputs": [],
      "source": [
        "# Plot a pie chart\n",
        "fig, ax = plt.subplots()\n",
        "wedges, texts, autotexts = ax.pie(value_counts, autopct='%1.1f%%', startangle=90, textprops=dict(color=\"w\"))\n",
        "\n",
        "# Add count labels to each slice\n",
        "for i, wedge in enumerate(wedges):\n",
        "    count_label = f\"{value_counts[i]}\"\n",
        "    plt.setp(autotexts[i], text=f\"{autotexts[i].get_text()}\\n({count_label})\")\n",
        "\n",
        "# Legend\n",
        "custom_labels = ['Vuln not detected', 'Vuln detected']\n",
        "ax.legend(wedges, custom_labels, loc='upper right', bbox_to_anchor=(1.1, 1))\n",
        "ax.set_title('Distribution of vuln functions')\n",
        "ax.axis('equal')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUxwaljeNFR-"
      },
      "source": [
        "# Correlation Plot\n",
        "We'll compute the correlation between each pair of variables in the pre-processed data. Correlation is a measure of how linearly related two variables are to each other. We'll then visualize the correlations in a heatmap.\n",
        "\n",
        "Since there are a lot of features, we can improve our visualization by filtering for values above (or below) a threshold (0.7) before plotting it\n",
        "\n",
        "Notice that our feature of interest ('vuln') does not have strong positive or negative correlations with the other features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0h8KZl41kbZj"
      },
      "outputs": [],
      "source": [
        "###########################\n",
        "###########################\n",
        "# STUDENTS ADD CODE HERE TO:\n",
        "# Use the pandas method corr() to compute pairwise correlations\n",
        "# between all engineered features\n",
        "correlations = numeric_df.corr()\n",
        "###########################\n",
        "###########################\n",
        "correlations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcJrBMxJk29f"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "###########################\n",
        "###########################\n",
        "# STUDENTS ADD CODE HERE TO:\n",
        "# Find values with strong positive or negative correlation and omit the values in the diagonal (feature correlated with itself)\n",
        "# Set a threshold of your choosing to show only values that are at or more correlated (positively and negatively)\n",
        "threshold = 0.7\n",
        "###########################\n",
        "###########################\n",
        "\n",
        "high_corr = correlations[((correlations < -threshold) | (correlations > threshold) ) & (correlations != 1)]\n",
        "\n",
        "# Plot the filtered correlation matrix\n",
        "plt.figure(figsize=(18, 12))\n",
        "sns.heatmap(high_corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
        "plt.title('Filtered Correlation Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtJTPihTNFR-"
      },
      "source": [
        "# Train and Classify\n",
        "\n",
        "We will now train a classification model called 'K Nearest Neighbors' (k -NN). The goal of this model is to be able to learn features (and the appropriate values) that are indicative of a function's vulnerability. \n",
        "\n",
        "\n",
        "k-NN identifies outliers by identifying a point in the dataset, finding its k nearest neigbhors using a distance measure and using a majority vote to determine the class to be assigned to that point. This learned state is used to predict new points.  \n",
        "\n",
        "### Key Parameters:\n",
        "\n",
        "* **`k`**: The number of nearest neighbors to consider for each data point.\n",
        "\n",
        "### Steps:\n",
        "\n",
        "1. **Standardize Features**: The data is standardized using `StandardScaler` for consistent unit variance.\n",
        "2. **Fit k-NN** on the training data to learn the distribution of the points.\n",
        "3. **Compute Average Distances** for each test sample to its `k` nearest neighbors in the training set.\n",
        "4. **Evaluate the Results** using a custom evaluation function.\n",
        "\n",
        "\n",
        "The model's performance is assessed using a confusion matrix, which indicates the true positive, true negative, false positive, and false negative classification rates.\n",
        "\n",
        "Additionally the following metrics give an insight into the model's strength/weaknesses:\n",
        "- Accuracy: Determines the ration of true positives + true negatives over the entire dataset. For imblanaced datasets, accuracy will be higher given that the model will do well by just guessing the majority class all the time.\n",
        "- Precision: Of the records predicted to be vulnerable, how many are truly vulnerable.\n",
        "- Recall: Of the total vulnerable records, how many did the model predict as vulnerable.\n",
        "- F1 score: Is the harmonic mean of the precision and recall. For imablanced datasets, it provides a better measure of the model's performance vs accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgSe9ye6pPwe"
      },
      "source": [
        "## Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkGqIEmp3zXW"
      },
      "outputs": [],
      "source": [
        "# Features to train (remove labels)\n",
        "X = numeric_df.drop('vuln', axis=1)\n",
        "# Class labels\n",
        "y = numeric_df.vuln.values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uw5Q6m03BV28"
      },
      "source": [
        "# Model Training and Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMmOzeP3NFR_"
      },
      "source": [
        "# BREAKDOWN STEPS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Data Preparation and Scaling\n",
        "\n",
        "\n",
        "X_scaled = pd.DataFrame(StandardScaler().fit_transform(X), columns=X.columns)\n",
        "\n",
        "X_selected = X_scaled.copy(deep=True)\n",
        "print(f\"Shape of scaled features: {X_selected.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Train-Test Split\n",
        "\n",
        "# Set the test size\n",
        "test_size = 0.2\n",
        "\n",
        "# Split the data into test and train sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=test_size, stratify=y, random_state=123)\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]}\")\n",
        "print(f\"Test set size: {X_test.shape[0]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Create and Train KNN Model\n",
        "\n",
        "# Set the number of neighbors\n",
        "k = 5\n",
        "\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=k, weights='distance')\n",
        "\n",
        "\n",
        "###########################\n",
        "###########################\n",
        "# STUDENTS ADD CODE HERE TO:\n",
        "# Train the KNN model on the training data\n",
        "knn.fit(X_train, y_train)\n",
        "###########################\n",
        "###########################\n",
        "\n",
        "print(f\"KNN model trained with k={k}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4: Make Predictions\n",
        "\n",
        "###########################\n",
        "###########################\n",
        "# STUDENTS ADD CODE HERE TO:\n",
        "# Use the trained model to predict labels for the test set\n",
        "y_pred = knn.predict(X_test)\n",
        "###########################\n",
        "###########################\n",
        "\n",
        "print(f\"Predictions made on {len(y_pred)} test samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5: Evaluate Model Performance\n",
        "\n",
        "# Define class labels\n",
        "labels = ['vuln not detected', 'vuln detected']\n",
        "\n",
        "\n",
        "(ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred), display_labels=labels)).plot()\n",
        "\n",
        "plt.title(f'KNN Confusion Matrix (k={k})')\n",
        "plt.show()\n",
        "\n",
        "print(classification_report(y_test, y_pred, target_names=labels))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PLAY WITH K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLRMkest36TQ"
      },
      "outputs": [],
      "source": [
        "# Widget elements\n",
        "col_list = list(X.columns) + ([\"all\"])\n",
        "k_slider = widgets.IntSlider(value=1, min=1, max=20, step=1, description='K')\n",
        "traintestsplit_slider = widgets.FloatSlider(value=0.3,min=0.1, max=0.9, step=0.1, description='Test Split')\n",
        "feature1_dropdown = widgets.Dropdown(options=col_list, value='cc', description='Feature 1')\n",
        "feature2_dropdown = widgets.Dropdown(options=col_list, value='ccl', description='Feature 2')\n",
        "\n",
        "update_button = widgets.Button(description='Update')\n",
        "\n",
        "# Display the widgets\n",
        "#display(k_slider, traintestsplit_slider, feature1_dropdown, feature2_dropdown, update_button)\n",
        "\n",
        "# Function to update the plot\n",
        "def update_plot(k, train_test_split_ratio, feature1, feature2):\n",
        "    # Scale data\n",
        "    X_scaled = pd.DataFrame(StandardScaler().fit_transform(X), columns=numeric_df.drop('vuln', axis=1).columns)\n",
        "    if feature1 == 'all' or feature2 == 'all':\n",
        "      X_selected = X_scaled.values\n",
        "    else:\n",
        "      X_selected = X_scaled[[feature1, feature2]].values\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=train_test_split_ratio, stratify=y, random_state=123)\n",
        "\n",
        "    knn = KNeighborsClassifier(n_neighbors=k, weights='distance')\n",
        "    knn.fit(X_train, y_train)\n",
        "\n",
        "    # Predict class labels for the training data\n",
        "    y_pred = knn.predict(X_test)\n",
        "        \n",
        "    # Clear the previous plot\n",
        "    clear_output(wait=True)\n",
        "    # Display the widgets again\n",
        "    display(k_slider, traintestsplit_slider, feature1_dropdown, feature2_dropdown, update_button)\n",
        "\n",
        "    if feature1 == 'all' or feature2 == 'all':\n",
        "      labels = ['vuln not detected','vuln detected']\n",
        "      (ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test,y_pred), display_labels=labels)).plot()\n",
        "      print((classification_report(y_test, y_pred, target_names = labels)))\n",
        "    else:\n",
        "      # Plotting\n",
        "      plt.figure(figsize=(10, 6))\n",
        "      scatter = plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, edgecolor='k', s=20, label='Data points')\n",
        "\n",
        "      # Create a legend with unique class labels and their counts\n",
        "      handles, labels = scatter.legend_elements()\n",
        "      class_labels = ['vuln not detected', 'vuln detected']\n",
        "      counts = [np.sum(y_pred == i) for i in np.unique(y_pred)]\n",
        "      true_counts = [np.sum(y_test == i) for i in np.unique(y_test)]\n",
        "      legend_labels = [f'{class_labels[i]} ({counts[i]}/{true_counts[i]})' for i in range(len(class_labels))]\n",
        "\n",
        "      plt.legend(handles=handles, labels=legend_labels)\n",
        "\n",
        "      plt.title(f'KNN Classification (k={k})')\n",
        "      plt.xlabel(feature1)\n",
        "      plt.ylabel(feature2)\n",
        "      plt.show()   \n",
        "\n",
        "\n",
        "# Function to handle update button click\n",
        "def on_update_button_clicked(b):\n",
        "    update_plot(k_slider.value, traintestsplit_slider.value, feature1_dropdown.value, feature2_dropdown.value)\n",
        "\n",
        "# Connect the button click event to the handler\n",
        "update_button.on_click(on_update_button_clicked)\n",
        "\n",
        "# Initial plot\n",
        "update_plot(k_slider.value,traintestsplit_slider.value, feature1_dropdown.value, feature2_dropdown.value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "infosec",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
